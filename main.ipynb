{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folktables\n",
    "from folktables import ACSDataSource\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "#(Age) must be greater than 16 and less than 90, and (Person weight) must be greater than or equal to 1\n",
    "def employment_filter(data):\n",
    "    \"\"\"\n",
    "    Filters for the employment prediction task\n",
    "    \"\"\"\n",
    "    df = data\n",
    "    df = df[df['AGEP'] > 16]\n",
    "    df = df[df['AGEP'] < 90]\n",
    "    df = df[df['PWGTP'] >= 1]\n",
    "    return df\n",
    "ACSEmployment = folktables.BasicProblem(\n",
    "    features=[\n",
    "       'AGEP', #age; for range of values of features please check Appendix B.4 of\n",
    "           #Retiring Adult: New Datasets for Fair Machine Learning NeurIPS 2021 paper\n",
    "       'SCHL', #educational attainment\n",
    "       'MAR', #marital status\n",
    "       'RELP', #relationship\n",
    "       'DIS', #disability recode\n",
    "       'ESP', #employment status of parents\n",
    "       'CIT', #citizenship status\n",
    "       'MIG', #mobility status (lived here 1 year ago)\n",
    "       'MIL', #military service\n",
    "       'ANC', #ancestry recode\n",
    "       'NATIVITY', #nativity\n",
    "       'DEAR', #hearing difficulty\n",
    "       'DEYE', #vision difficulty\n",
    "       'DREM', #cognitive difficulty\n",
    "       'SEX', #sex\n",
    "       'RAC1P', #recoded detailed race code\n",
    "       'GCL', #grandparents living with grandchildren\n",
    "    ],\n",
    "    target='ESR', #employment status recode\n",
    "    target_transform=lambda x: x == 1,\n",
    "    group='DIS',\n",
    "    preprocess=employment_filter,\n",
    "    postprocess=lambda x: np.nan_to_num(x, -1),\n",
    ")\n",
    "data_source = ACSDataSource(survey_year='2018', horizon='1-Year', survey='person')\n",
    "acs_data = data_source.get_data(states=[\"FL\"], download=True) #data for Florida state\n",
    "features, label, group = ACSEmployment.df_to_numpy(acs_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Kotsi/Desktop/AI_Imperial/Ethics/Fairness-ML/my_env/lib/python3.11/site-packages/aif360/datasets/standard_dataset.py:143: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '1.0' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[pos, label_name] = favorable_label\n"
     ]
    }
   ],
   "source": [
    "from aif360.datasets import StandardDataset\n",
    "import pandas as pd\n",
    "data = pd.DataFrame(features, columns = ACSEmployment.features)\n",
    "data['label'] = label\n",
    "favorable_classes = [True]\n",
    "protected_attribute_names = [ACSEmployment.group]\n",
    "privileged_classes = np.array([[1]])\n",
    "data_for_aif = StandardDataset(data, 'label', favorable_classes = favorable_classes,\n",
    "                      protected_attribute_names = protected_attribute_names,\n",
    "                      privileged_classes = privileged_classes)\n",
    "privileged_groups = [{'DIS': 1}]\n",
    "unprivileged_groups = [{'DIS': 2}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1(a) - Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = data_for_aif.features.shape[0]\n",
    "train, test = data_for_aif.split([0.7], shuffle=True)\n",
    "splits = []\n",
    "for _ in range(5):\n",
    "    train, val = train.split([0.8], shuffle=True)\n",
    "    splits.append((train, val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1(b) - Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler  \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "def find_most_accurate_logistic_reg_model(splits, parameters):\n",
    "    final_scaler = None\n",
    "    max_accuracy = -float('inf')\n",
    "    best_model = None\n",
    "    for parameter in parameters:\n",
    "        accuracies = []\n",
    "        for train, val in splits:\n",
    "            scaler = StandardScaler()\n",
    "            X_train = scaler.fit_transform(train.features)\n",
    "            y_train = train.labels.ravel()\n",
    "\n",
    "            X_val = scaler.transform(val.features)\n",
    "            y_val = val.labels.ravel()\n",
    "            learner = LogisticRegression(C=parameter, solver='liblinear', random_state=1)\n",
    "            learner.fit(X_train, y_train)\n",
    "            predictions = learner.predict(X_val)\n",
    "            accuracy = sum((predictions==y_val) / len(y_val))\n",
    "            accuracies.append(accuracy)\n",
    "        mean_accuracy = sum(accuracies) / len(accuracies)\n",
    "        if mean_accuracy > max_accuracy:\n",
    "            max_accuracy = mean_accuracy\n",
    "            best_model = learner\n",
    "            final_scaler = scaler\n",
    "    return best_model, final_scaler, max_accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "final_model, final_scaler, final_accuracy = find_most_accurate_logistic_reg_model(splits=splits, parameters=parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model: LogisticRegression(C=0.1, random_state=1, solver='liblinear')\n",
      "\n",
      "Final Scaler: StandardScaler()\n",
      "\n",
      "Final Mean Accuracy on Val data : 0.751333899988068\n"
     ]
    }
   ],
   "source": [
    "print(f\"Final Model: {final_model}\\n\")\n",
    "print(f\"Final Scaler: {final_scaler}\\n\")\n",
    "print(f\"Final Mean Accuracy on Val data : {final_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the final model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6253460276847063"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from aif360.metrics import ClassificationMetric\n",
    "\n",
    "X_test = final_scaler.transform(test.features)\n",
    "y_test = test.labels.ravel()\n",
    "\n",
    "test_predictions = final_model.predict(X_test)\n",
    "\n",
    "test_pred = test.copy()\n",
    "test_pred.labels = test_predictions\n",
    "\n",
    "test_accuracy = sum((test_predictions == y_test) / len(y_test))\n",
    "\n",
    "metric = ClassificationMetric(test, test_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
    "eq_opp_diff = metric.equal_opportunity_difference()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final accuracy and fairness on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy on test data: 0.7534268409311323\n",
      "\n",
      "Fairness on test data using the equal opportunity difference metric: 0.6253460276847063\n"
     ]
    }
   ],
   "source": [
    "print(f\"Final accuracy on test data: {test_accuracy}\\n\")\n",
    "print(f\"Fairness on test data using the equal opportunity difference metric: {eq_opp_diff}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task(c) - Model selection based on fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_fair_logistic_reg_model(splits, parameters):\n",
    "    final_scaler = None\n",
    "    min_diff = float('inf')\n",
    "    best_model = None\n",
    "    for parameter in parameters:\n",
    "        fairnesses = []\n",
    "        for train, val in splits:\n",
    "            scaler = StandardScaler()\n",
    "            X_train = scaler.fit_transform(train.features)\n",
    "            y_train = train.labels.ravel()\n",
    "\n",
    "            X_val = scaler.transform(val.features)\n",
    "            y_val = val.labels.ravel()\n",
    "            learner = LogisticRegression(C=parameter, solver='liblinear', random_state=1)\n",
    "            learner.fit(X_train, y_train)\n",
    "            predictions = learner.predict(X_val)\n",
    "            val_pred = val.copy()\n",
    "            val_pred.labels = predictions\n",
    "            metric = ClassificationMetric(val, val_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
    "            fairness = abs(metric.equal_opportunity_difference())\n",
    "            fairnesses.append(fairness)\n",
    "            \n",
    "        mean_fairness = sum(fairnesses) / len(fairnesses)\n",
    "        if mean_fairness < min_diff:\n",
    "            min_diff = mean_fairness\n",
    "            best_model = learner\n",
    "            final_scaler = scaler\n",
    "    return best_model, final_scaler, min_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model, final_scaler, final_fairness = find_most_fair_logistic_reg_model(splits = splits, parameters=parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the final model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = final_scaler.transform(test.features)\n",
    "y_test = test.labels.ravel()\n",
    "\n",
    "test_predictions = final_model.predict(X_test)\n",
    "\n",
    "test_pred = test.copy()\n",
    "test_pred.labels = test_predictions\n",
    "\n",
    "test_accuracy = sum((test_predictions == y_test) / len(y_test))\n",
    "\n",
    "metric = ClassificationMetric(test, test_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
    "eq_opp_diff = metric.equal_opportunity_difference()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final accuracy and fairness on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy on test data: 0.7531678355119419\n",
      "\n",
      "Fairness on test data using the equal opportunity difference metric: 0.6227677730313754\n"
     ]
    }
   ],
   "source": [
    "print(f\"Final accuracy on test data: {test_accuracy}\\n\")\n",
    "print(f\"Fairness on test data using the equal opportunity difference metric: {eq_opp_diff}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
